04/23/2025 11:07:40 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
04/23/2025 11:07:40 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
batch_eval_metrics=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=no,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output_512_short_1/runs/Apr23_11-07-24_c08580337451,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=100,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=constant,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=20000.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=output_512_short_1,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=190,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=output_512_short_1/checkpoint-103000,
run_name=output_512_short_1,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=1000,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
04/23/2025 11:07:40 - WARNING - __main__ - You are instantiating a new config instance from scratch.
04/23/2025 11:07:40 - INFO - __main__ - Training new model from scratch - Total size=3.72M params
Config: GPT2Config {
  "activation_function": "gelu_new",
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_embd": 256,
  "n_head": 4,
  "n_inner": null,
  "n_layer": 4,
  "n_positions": 1504,
  "pad_token_id": 0,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 1374
}

Model: GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(1374, 256)
    (wpe): Embedding(1504, 256)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-3): 4 x GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2SdpaAttention(
          (c_attn): Conv1D(nf=768, nx=256)
          (c_proj): Conv1D(nf=256, nx=256)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D(nf=1024, nx=256)
          (c_proj): Conv1D(nf=256, nx=1024)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=256, out_features=1374, bias=False)
)
04/23/2025 11:07:40 - WARNING - __main__ - The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can override this default with `--block_size xxx`.
04/23/2025 11:07:41 - WARNING - __main__ - Process rank: 5, device: cuda:5, n_gpu: 1distributed training: True, 16-bits training: True
04/23/2025 11:07:41 - WARNING - __main__ - Process rank: 4, device: cuda:4, n_gpu: 1distributed training: True, 16-bits training: True
04/23/2025 11:07:41 - WARNING - __main__ - Process rank: 2, device: cuda:2, n_gpu: 1distributed training: True, 16-bits training: True
04/23/2025 11:07:41 - WARNING - __main__ - Process rank: 3, device: cuda:3, n_gpu: 1distributed training: True, 16-bits training: True
04/23/2025 11:07:41 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True
04/23/2025 11:07:41 - WARNING - __main__ - You are instantiating a new config instance from scratch.
04/23/2025 11:07:41 - WARNING - trainer - There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
04/23/2025 11:07:41 - WARNING - __main__ - You are instantiating a new config instance from scratch.
04/23/2025 11:07:41 - WARNING - __main__ - You are instantiating a new config instance from scratch.
04/23/2025 11:07:42 - WARNING - __main__ - You are instantiating a new config instance from scratch.
Config: GPT2Config {
  "activation_function": "gelu_new",
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_embd": 256,
  "n_head": 4,
  "n_inner": null,
  "n_layer": 4,
  "n_positions": 1504,
  "pad_token_id": 0,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 1374
}

Model: GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(1374, 256)
    (wpe): Embedding(1504, 256)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-3): 4 x GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2SdpaAttention(
          (c_attn): Conv1D(nf=768, nx=256)
          (c_proj): Conv1D(nf=256, nx=256)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D(nf=1024, nx=256)
          (c_proj): Conv1D(nf=256, nx=1024)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=256, out_features=1374, bias=False)
)
04/23/2025 11:07:42 - WARNING - __main__ - The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can override this default with `--block_size xxx`.
04/23/2025 11:07:42 - WARNING - __main__ - You are instantiating a new config instance from scratch.
Config: GPT2Config {
  "activation_function": "gelu_new",
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_embd": 256,
  "n_head": 4,
  "n_inner": null,
  "n_layer": 4,
  "n_positions": 1504,
  "pad_token_id": 0,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 1374
}

Model: GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(1374, 256)
    (wpe): Embedding(1504, 256)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-3): 4 x GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2SdpaAttention(
          (c_attn): Conv1D(nf=768, nx=256)
          (c_proj): Conv1D(nf=256, nx=256)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D(nf=1024, nx=256)
          (c_proj): Conv1D(nf=256, nx=1024)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=256, out_features=1374, bias=False)
)
04/23/2025 11:07:42 - WARNING - __main__ - The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can override this default with `--block_size xxx`.
Config: GPT2Config {
  "activation_function": "gelu_new",
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_embd": 256,
  "n_head": 4,
  "n_inner": null,
  "n_layer": 4,
  "n_positions": 1504,
  "pad_token_id": 0,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 1374
}

Model: GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(1374, 256)
    (wpe): Embedding(1504, 256)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-3): 4 x GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2SdpaAttention(
          (c_attn): Conv1D(nf=768, nx=256)
          (c_proj): Conv1D(nf=256, nx=256)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D(nf=1024, nx=256)
          (c_proj): Conv1D(nf=256, nx=1024)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=256, out_features=1374, bias=False)
)
04/23/2025 11:07:42 - WARNING - __main__ - The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can override this default with `--block_size xxx`.
04/23/2025 11:07:42 - WARNING - trainer - There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
Config: GPT2Config {
  "activation_function": "gelu_new",
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_embd": 256,
  "n_head": 4,
  "n_inner": null,
  "n_layer": 4,
  "n_positions": 1504,
  "pad_token_id": 0,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 1374
}

Model: GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(1374, 256)
    (wpe): Embedding(1504, 256)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-3): 4 x GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2SdpaAttention(
          (c_attn): Conv1D(nf=768, nx=256)
          (c_proj): Conv1D(nf=256, nx=256)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D(nf=1024, nx=256)
          (c_proj): Conv1D(nf=256, nx=1024)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=256, out_features=1374, bias=False)
)
04/23/2025 11:07:42 - WARNING - __main__ - The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can override this default with `--block_size xxx`.
Config: GPT2Config {
  "activation_function": "gelu_new",
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_embd": 256,
  "n_head": 4,
  "n_inner": null,
  "n_layer": 4,
  "n_positions": 1504,
  "pad_token_id": 0,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 1374
}

Model: GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(1374, 256)
    (wpe): Embedding(1504, 256)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-3): 4 x GPT2Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2SdpaAttention(
          (c_attn): Conv1D(nf=768, nx=256)
          (c_proj): Conv1D(nf=256, nx=256)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D(nf=1024, nx=256)
          (c_proj): Conv1D(nf=256, nx=1024)
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=256, out_features=1374, bias=False)
)
04/23/2025 11:07:42 - WARNING - __main__ - The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can override this default with `--block_size xxx`.
04/23/2025 11:07:42 - WARNING - trainer - There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
04/23/2025 11:07:42 - WARNING - trainer - There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
04/23/2025 11:07:42 - WARNING - trainer - There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
04/23/2025 11:07:42 - WARNING - trainer - There were missing keys in the checkpoint model loaded: ['lm_head.weight'].
{'loss': 0.0004, 'loss1': 4.4963, 'loss2': 0.0, 'grad_norm': 0.0006259388173930347, 'learning_rate': 5e-05, 'epoch': 1185.06}
{'loss': 0.0004, 'loss1': 4.2977, 'loss2': 0.0, 'grad_norm': 0.013801285065710545, 'learning_rate': 5e-05, 'epoch': 1186.21}
{'loss': 0.0004, 'loss1': 4.2682, 'loss2': 0.0, 'grad_norm': 0.0005272902199067175, 'learning_rate': 5e-05, 'epoch': 1187.36}
{'loss': 0.0004, 'loss1': 4.1572, 'loss2': 0.0, 'grad_norm': 0.0014416839694604278, 'learning_rate': 5e-05, 'epoch': 1188.51}
{'loss': 0.0004, 'loss1': 3.6341, 'loss2': 0.0, 'grad_norm': 0.006424279883503914, 'learning_rate': 5e-05, 'epoch': 1189.66}
{'loss': 0.0005, 'loss1': 4.7622, 'loss2': 0.0, 'grad_norm': 0.0010953402379527688, 'learning_rate': 5e-05, 'epoch': 1190.8}
{'loss': 0.0004, 'loss1': 3.6861, 'loss2': 0.0, 'grad_norm': 0.004982268437743187, 'learning_rate': 5e-05, 'epoch': 1191.95}
{'loss': 0.0004, 'loss1': 4.0087, 'loss2': 0.0, 'grad_norm': 0.009130291640758514, 'learning_rate': 5e-05, 'epoch': 1193.1}
{'loss': 0.0004, 'loss1': 4.0075, 'loss2': 0.0, 'grad_norm': 0.019498232752084732, 'learning_rate': 5e-05, 'epoch': 1194.25}
{'loss': 0.0004, 'loss1': 4.1164, 'loss2': 0.0, 'grad_norm': 0.002524418756365776, 'learning_rate': 5e-05, 'epoch': 1195.4}
{'loss': 0.0004, 'loss1': 4.1654, 'loss2': 0.0, 'grad_norm': 0.0007923186058178544, 'learning_rate': 5e-05, 'epoch': 1196.55}
{'loss': 0.0004, 'loss1': 4.2705, 'loss2': 0.0, 'grad_norm': 0.0034088813699781895, 'learning_rate': 5e-05, 'epoch': 1197.7}
{'loss': 0.0004, 'loss1': 3.6275, 'loss2': 0.0, 'grad_norm': 0.002300018211826682, 'learning_rate': 5e-05, 'epoch': 1198.85}
{'loss': 0.0004, 'loss1': 3.8713, 'loss2': 0.0, 'grad_norm': 0.0008533193613402545, 'learning_rate': 5e-05, 'epoch': 1200.0}
{'loss': 0.0004, 'loss1': 4.2273, 'loss2': 0.0, 'grad_norm': 0.0006293538026511669, 'learning_rate': 5e-05, 'epoch': 1201.15}
{'loss': 0.0004, 'loss1': 4.0586, 'loss2': 0.0, 'grad_norm': 0.004740552976727486, 'learning_rate': 5e-05, 'epoch': 1202.3}
{'loss': 0.0004, 'loss1': 4.4479, 'loss2': 0.0, 'grad_norm': 0.0009743188857100904, 'learning_rate': 5e-05, 'epoch': 1203.45}
{'loss': 0.0005, 'loss1': 4.7853, 'loss2': 0.0, 'grad_norm': 0.011428856290876865, 'learning_rate': 5e-05, 'epoch': 1204.6}
{'loss': 0.0005, 'loss1': 5.0428, 'loss2': 0.0, 'grad_norm': 0.003869033884257078, 'learning_rate': 5e-05, 'epoch': 1205.75}
{'loss': 0.0004, 'loss1': 4.2909, 'loss2': 0.0, 'grad_norm': 0.004415258765220642, 'learning_rate': 5e-05, 'epoch': 1206.9}
{'loss': 0.0004, 'loss1': 4.2934, 'loss2': 0.0, 'grad_norm': 0.00073364342097193, 'learning_rate': 5e-05, 'epoch': 1208.05}
{'loss': 0.0004, 'loss1': 4.3492, 'loss2': 0.0, 'grad_norm': 0.0076078688725829124, 'learning_rate': 5e-05, 'epoch': 1209.2}
{'loss': 0.0004, 'loss1': 4.0766, 'loss2': 0.0, 'grad_norm': 0.005048804450780153, 'learning_rate': 5e-05, 'epoch': 1210.34}
{'loss': 0.0004, 'loss1': 4.469, 'loss2': 0.0, 'grad_norm': 0.0008720489568077028, 'learning_rate': 5e-05, 'epoch': 1211.49}
{'loss': 0.0003, 'loss1': 3.4642, 'loss2': 0.0, 'grad_norm': 0.002513215411454439, 'learning_rate': 5e-05, 'epoch': 1212.64}
{'loss': 0.0004, 'loss1': 4.1408, 'loss2': 0.0, 'grad_norm': 0.015656741335988045, 'learning_rate': 5e-05, 'epoch': 1213.79}
{'loss': 0.0004, 'loss1': 3.8621, 'loss2': 0.0, 'grad_norm': 0.01733468845486641, 'learning_rate': 5e-05, 'epoch': 1214.94}
{'loss': 0.0004, 'loss1': 4.1396, 'loss2': 0.0, 'grad_norm': 0.016159702092409134, 'learning_rate': 5e-05, 'epoch': 1216.09}
{'loss': 0.0004, 'loss1': 3.5811, 'loss2': 0.0, 'grad_norm': 0.002641173778101802, 'learning_rate': 5e-05, 'epoch': 1217.24}
{'loss': 0.0003, 'loss1': 3.434, 'loss2': 0.0, 'grad_norm': 0.0076294331811368465, 'learning_rate': 5e-05, 'epoch': 1218.39}
{'loss': 0.0004, 'loss1': 4.1358, 'loss2': 0.0, 'grad_norm': 0.011573845520615578, 'learning_rate': 5e-05, 'epoch': 1219.54}
{'loss': 0.0005, 'loss1': 4.5396, 'loss2': 0.0, 'grad_norm': 0.002183307195082307, 'learning_rate': 5e-05, 'epoch': 1220.69}
{'loss': 0.0004, 'loss1': 3.7045, 'loss2': 0.0, 'grad_norm': 0.00022194701887201518, 'learning_rate': 5e-05, 'epoch': 1221.84}
{'loss': 0.0004, 'loss1': 3.7332, 'loss2': 0.0, 'grad_norm': 0.0147358113899827, 'learning_rate': 5e-05, 'epoch': 1222.99}
{'loss': 0.0005, 'loss1': 4.6443, 'loss2': 0.0, 'grad_norm': 0.0071366215124726295, 'learning_rate': 5e-05, 'epoch': 1224.14}
{'loss': 0.0005, 'loss1': 4.9379, 'loss2': 0.0, 'grad_norm': 0.006616475060582161, 'learning_rate': 5e-05, 'epoch': 1225.29}
{'loss': 0.0003, 'loss1': 3.4967, 'loss2': 0.0, 'grad_norm': 0.012906786985695362, 'learning_rate': 5e-05, 'epoch': 1226.44}
{'loss': 0.0004, 'loss1': 4.4956, 'loss2': 0.0, 'grad_norm': 0.0017799880588427186, 'learning_rate': 5e-05, 'epoch': 1227.59}
{'loss': 0.0004, 'loss1': 4.1287, 'loss2': 0.0, 'grad_norm': 0.00017749189282767475, 'learning_rate': 5e-05, 'epoch': 1228.74}
{'loss': 0.0004, 'loss1': 4.0746, 'loss2': 0.0, 'grad_norm': 0.013244425877928734, 'learning_rate': 5e-05, 'epoch': 1229.89}
{'loss': 0.0005, 'loss1': 4.5689, 'loss2': 0.0, 'grad_norm': 0.0002502096467651427, 'learning_rate': 5e-05, 'epoch': 1231.03}
{'loss': 0.0004, 'loss1': 4.1651, 'loss2': 0.0, 'grad_norm': 0.002543018199503422, 'learning_rate': 5e-05, 'epoch': 1232.18}
{'loss': 0.0004, 'loss1': 3.6752, 'loss2': 0.0, 'grad_norm': 0.002666589803993702, 'learning_rate': 5e-05, 'epoch': 1233.33}
{'loss': 0.0004, 'loss1': 3.7416, 'loss2': 0.0, 'grad_norm': 0.0002829483710229397, 'learning_rate': 5e-05, 'epoch': 1234.48}
{'loss': 0.0004, 'loss1': 3.89, 'loss2': 0.0, 'grad_norm': 0.011461785063147545, 'learning_rate': 5e-05, 'epoch': 1235.63}
{'loss': 0.0004, 'loss1': 4.224, 'loss2': 0.0, 'grad_norm': 0.0024078895803540945, 'learning_rate': 5e-05, 'epoch': 1236.78}
{'loss': 0.0004, 'loss1': 4.4838, 'loss2': 0.0, 'grad_norm': 0.0009656071779318154, 'learning_rate': 5e-05, 'epoch': 1237.93}
{'loss': 0.0005, 'loss1': 4.5829, 'loss2': 0.0, 'grad_norm': 0.007050913292914629, 'learning_rate': 5e-05, 'epoch': 1239.08}
{'loss': 0.0004, 'loss1': 3.6194, 'loss2': 0.0, 'grad_norm': 0.0003073106345254928, 'learning_rate': 5e-05, 'epoch': 1240.23}
{'loss': 0.0004, 'loss1': 4.0613, 'loss2': 0.0, 'grad_norm': 0.0027246978133916855, 'learning_rate': 5e-05, 'epoch': 1241.38}
{'loss': 0.0004, 'loss1': 3.6249, 'loss2': 0.0, 'grad_norm': 0.0020748216193169355, 'learning_rate': 5e-05, 'epoch': 1242.53}
{'loss': 0.0005, 'loss1': 4.5306, 'loss2': 0.0, 'grad_norm': 0.037588976323604584, 'learning_rate': 5e-05, 'epoch': 1243.68}
{'loss': 0.0004, 'loss1': 4.4584, 'loss2': 0.0, 'grad_norm': 0.0210098996758461, 'learning_rate': 5e-05, 'epoch': 1244.83}
{'loss': 0.0004, 'loss1': 3.6204, 'loss2': 0.0, 'grad_norm': 0.0034135000314563513, 'learning_rate': 5e-05, 'epoch': 1245.98}
{'loss': 0.0004, 'loss1': 4.3018, 'loss2': 0.0, 'grad_norm': 0.001485221553593874, 'learning_rate': 5e-05, 'epoch': 1247.13}
{'loss': 0.0005, 'loss1': 4.7321, 'loss2': 0.0, 'grad_norm': 0.0012941715540364385, 'learning_rate': 5e-05, 'epoch': 1248.28}
{'loss': 0.0004, 'loss1': 3.9408, 'loss2': 0.0, 'grad_norm': 0.0036120638251304626, 'learning_rate': 5e-05, 'epoch': 1249.43}
{'loss': 0.0005, 'loss1': 5.3944, 'loss2': 0.0, 'grad_norm': 0.011323670856654644, 'learning_rate': 5e-05, 'epoch': 1250.57}
{'loss': 0.0004, 'loss1': 4.3112, 'loss2': 0.0, 'grad_norm': 0.0019839894957840443, 'learning_rate': 5e-05, 'epoch': 1251.72}
{'loss': 0.0004, 'loss1': 4.4403, 'loss2': 0.0, 'grad_norm': 0.005989269819110632, 'learning_rate': 5e-05, 'epoch': 1252.87}
{'loss': 0.0004, 'loss1': 4.0822, 'loss2': 0.0, 'grad_norm': 0.009873250499367714, 'learning_rate': 5e-05, 'epoch': 1254.02}
{'loss': 0.0004, 'loss1': 4.1277, 'loss2': 0.0, 'grad_norm': 0.0010716303950175643, 'learning_rate': 5e-05, 'epoch': 1255.17}
{'loss': 0.0005, 'loss1': 4.5823, 'loss2': 0.0, 'grad_norm': 0.026028258726000786, 'learning_rate': 5e-05, 'epoch': 1256.32}
{'loss': 0.0004, 'loss1': 4.0832, 'loss2': 0.0, 'grad_norm': 0.009786112233996391, 'learning_rate': 5e-05, 'epoch': 1257.47}
{'loss': 0.0004, 'loss1': 4.3704, 'loss2': 0.0, 'grad_norm': 0.008732513524591923, 'learning_rate': 5e-05, 'epoch': 1258.62}
{'loss': 0.0004, 'loss1': 4.1161, 'loss2': 0.0, 'grad_norm': 0.009372484870254993, 'learning_rate': 5e-05, 'epoch': 1259.77}
{'loss': 0.0004, 'loss1': 4.0128, 'loss2': 0.0, 'grad_norm': 0.004625685513019562, 'learning_rate': 5e-05, 'epoch': 1260.92}
{'loss': 0.0004, 'loss1': 3.6908, 'loss2': 0.0, 'grad_norm': 0.003024239558726549, 'learning_rate': 5e-05, 'epoch': 1262.07}
{'loss': 0.0004, 'loss1': 4.1644, 'loss2': 0.0, 'grad_norm': 0.0003820722340606153, 'learning_rate': 5e-05, 'epoch': 1263.22}
{'loss': 0.0003, 'loss1': 3.4224, 'loss2': 0.0, 'grad_norm': 0.002974222181364894, 'learning_rate': 5e-05, 'epoch': 1264.37}
{'loss': 0.0003, 'loss1': 3.4849, 'loss2': 0.0, 'grad_norm': 0.0042615593411028385, 'learning_rate': 5e-05, 'epoch': 1265.52}
{'loss': 0.0004, 'loss1': 4.0851, 'loss2': 0.0, 'grad_norm': 0.00863666832447052, 'learning_rate': 5e-05, 'epoch': 1266.67}
{'loss': 0.0003, 'loss1': 3.3403, 'loss2': 0.0, 'grad_norm': 0.008489652536809444, 'learning_rate': 5e-05, 'epoch': 1267.82}
{'loss': 0.0003, 'loss1': 3.4937, 'loss2': 0.0, 'grad_norm': 0.00027947474154643714, 'learning_rate': 5e-05, 'epoch': 1268.97}
{'loss': 0.0004, 'loss1': 3.7235, 'loss2': 0.0, 'grad_norm': 0.005310936365276575, 'learning_rate': 5e-05, 'epoch': 1270.11}
{'loss': 0.0004, 'loss1': 3.7047, 'loss2': 0.0, 'grad_norm': 0.006341997999697924, 'learning_rate': 5e-05, 'epoch': 1271.26}
{'loss': 0.0004, 'loss1': 4.25, 'loss2': 0.0, 'grad_norm': 0.0012547897640615702, 'learning_rate': 5e-05, 'epoch': 1272.41}
{'loss': 0.0004, 'loss1': 3.5662, 'loss2': 0.0, 'grad_norm': 0.0014207115164026618, 'learning_rate': 5e-05, 'epoch': 1273.56}
{'loss': 0.0004, 'loss1': 3.9668, 'loss2': 0.0, 'grad_norm': 0.005949903279542923, 'learning_rate': 5e-05, 'epoch': 1274.71}
{'loss': 0.0004, 'loss1': 4.4568, 'loss2': 0.0, 'grad_norm': 0.013215950690209866, 'learning_rate': 5e-05, 'epoch': 1275.86}
{'loss': 0.0004, 'loss1': 3.9024, 'loss2': 0.0, 'grad_norm': 0.03702053427696228, 'learning_rate': 5e-05, 'epoch': 1277.01}
{'loss': 0.0004, 'loss1': 3.9945, 'loss2': 0.0, 'grad_norm': 0.001051376573741436, 'learning_rate': 5e-05, 'epoch': 1278.16}
{'loss': 0.0004, 'loss1': 3.8333, 'loss2': 0.0, 'grad_norm': 0.03056127019226551, 'learning_rate': 5e-05, 'epoch': 1279.31}
{'loss': 0.0004, 'loss1': 3.6403, 'loss2': 0.0, 'grad_norm': 0.02791309915482998, 'learning_rate': 5e-05, 'epoch': 1280.46}
{'loss': 0.0004, 'loss1': 3.7944, 'loss2': 0.0, 'grad_norm': 0.0005174704128876328, 'learning_rate': 5e-05, 'epoch': 1281.61}
{'loss': 0.0003, 'loss1': 3.4027, 'loss2': 0.0, 'grad_norm': 0.004217574838548899, 'learning_rate': 5e-05, 'epoch': 1282.76}
{'loss': 0.0004, 'loss1': 4.3371, 'loss2': 0.0, 'grad_norm': 0.0006727883592247963, 'learning_rate': 5e-05, 'epoch': 1283.91}
{'loss': 0.0005, 'loss1': 4.7407, 'loss2': 0.0, 'grad_norm': 0.013923075050115585, 'learning_rate': 5e-05, 'epoch': 1285.06}
{'loss': 0.0004, 'loss1': 3.9238, 'loss2': 0.0, 'grad_norm': 0.018144141882658005, 'learning_rate': 5e-05, 'epoch': 1286.21}
{'loss': 0.0004, 'loss1': 4.4412, 'loss2': 0.0, 'grad_norm': 0.007813870906829834, 'learning_rate': 5e-05, 'epoch': 1287.36}
{'loss': 0.0004, 'loss1': 4.2546, 'loss2': 0.0, 'grad_norm': 0.00554288225248456, 'learning_rate': 5e-05, 'epoch': 1288.51}
{'loss': 0.0004, 'loss1': 3.509, 'loss2': 0.0, 'grad_norm': 0.005313421133905649, 'learning_rate': 5e-05, 'epoch': 1289.66}
{'loss': 0.0005, 'loss1': 4.5796, 'loss2': 0.0, 'grad_norm': 0.034324005246162415, 'learning_rate': 5e-05, 'epoch': 1290.8}
{'loss': 0.0004, 'loss1': 4.223, 'loss2': 0.0, 'grad_norm': 0.018435725942254066, 'learning_rate': 5e-05, 'epoch': 1291.95}
{'loss': 0.0004, 'loss1': 3.6115, 'loss2': 0.0, 'grad_norm': 0.00046757966629229486, 'learning_rate': 5e-05, 'epoch': 1293.1}
{'loss': 0.0004, 'loss1': 3.6723, 'loss2': 0.0, 'grad_norm': 0.0003570699773263186, 'learning_rate': 5e-05, 'epoch': 1294.25}
{'loss': 0.0003, 'loss1': 3.3884, 'loss2': 0.0, 'grad_norm': 0.0024650723207741976, 'learning_rate': 5e-05, 'epoch': 1295.4}
{'loss': 0.0004, 'loss1': 3.668, 'loss2': 0.0, 'grad_norm': 0.007200462277978659, 'learning_rate': 5e-05, 'epoch': 1296.55}
{'loss': 0.0004, 'loss1': 4.1824, 'loss2': 0.0, 'grad_norm': 0.018812604248523712, 'learning_rate': 5e-05, 'epoch': 1297.7}
{'loss': 0.0004, 'loss1': 4.2815, 'loss2': 0.0, 'grad_norm': 0.004486184101551771, 'learning_rate': 5e-05, 'epoch': 1298.85}
{'loss': 0.0004, 'loss1': 4.2347, 'loss2': 0.0, 'grad_norm': 0.0003189857816323638, 'learning_rate': 5e-05, 'epoch': 1300.0}
{'loss': 0.0004, 'loss1': 3.937, 'loss2': 0.0, 'grad_norm': 0.00027487624902278185, 'learning_rate': 5e-05, 'epoch': 1301.15}
{'loss': 0.0004, 'loss1': 3.709, 'loss2': 0.0, 'grad_norm': 0.010270814411342144, 'learning_rate': 5e-05, 'epoch': 1302.3}
{'loss': 0.0004, 'loss1': 4.0449, 'loss2': 0.0, 'grad_norm': 0.0050444030202925205, 'learning_rate': 5e-05, 'epoch': 1303.45}
{'loss': 0.0004, 'loss1': 4.4254, 'loss2': 0.0, 'grad_norm': 0.040844641625881195, 'learning_rate': 5e-05, 'epoch': 1304.6}
{'loss': 0.0003, 'loss1': 3.395, 'loss2': 0.0, 'grad_norm': 9.513297118246555e-05, 'learning_rate': 5e-05, 'epoch': 1305.75}
{'loss': 0.0004, 'loss1': 4.4442, 'loss2': 0.0, 'grad_norm': 0.001081146183423698, 'learning_rate': 5e-05, 'epoch': 1306.9}
{'loss': 0.0004, 'loss1': 4.1921, 'loss2': 0.0, 'grad_norm': 0.02006269060075283, 'learning_rate': 5e-05, 'epoch': 1308.05}
{'loss': 0.0004, 'loss1': 3.9851, 'loss2': 0.0, 'grad_norm': 0.03501258045434952, 'learning_rate': 5e-05, 'epoch': 1309.2}
{'loss': 0.0004, 'loss1': 3.8872, 'loss2': 0.0, 'grad_norm': 0.0032575952354818583, 'learning_rate': 5e-05, 'epoch': 1310.34}
{'loss': 0.0004, 'loss1': 3.7695, 'loss2': 0.0, 'grad_norm': 0.008400898426771164, 'learning_rate': 5e-05, 'epoch': 1311.49}
{'loss': 0.0005, 'loss1': 4.5881, 'loss2': 0.0, 'grad_norm': 0.006141971796751022, 'learning_rate': 5e-05, 'epoch': 1312.64}
{'loss': 0.0004, 'loss1': 3.7125, 'loss2': 0.0, 'grad_norm': 0.00021465035388246179, 'learning_rate': 5e-05, 'epoch': 1313.79}
{'loss': 0.0004, 'loss1': 4.2289, 'loss2': 0.0, 'grad_norm': 0.008970013819634914, 'learning_rate': 5e-05, 'epoch': 1314.94}
{'loss': 0.0004, 'loss1': 3.9597, 'loss2': 0.0, 'grad_norm': 0.004145489539951086, 'learning_rate': 5e-05, 'epoch': 1316.09}
{'loss': 0.0004, 'loss1': 4.3857, 'loss2': 0.0, 'grad_norm': 0.020807063207030296, 'learning_rate': 5e-05, 'epoch': 1317.24}
{'loss': 0.0004, 'loss1': 3.8132, 'loss2': 0.0, 'grad_norm': 0.0024121725000441074, 'learning_rate': 5e-05, 'epoch': 1318.39}
{'loss': 0.0004, 'loss1': 3.8536, 'loss2': 0.0, 'grad_norm': 0.003930346574634314, 'learning_rate': 5e-05, 'epoch': 1319.54}
{'loss': 0.0004, 'loss1': 4.2266, 'loss2': 0.0, 'grad_norm': 0.03121902234852314, 'learning_rate': 5e-05, 'epoch': 1320.69}
{'loss': 0.0004, 'loss1': 3.8738, 'loss2': 0.0, 'grad_norm': 0.007125030737370253, 'learning_rate': 5e-05, 'epoch': 1321.84}
