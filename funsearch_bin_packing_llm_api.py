import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import funsearch
from implementation import config
from implementation import sampler
from implementation import evaluator_accelerate
from implementation import evaluator
from implementation import code_manipulation
import bin_packing_utils

import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import sampler
import requests
import time
from concurrent.futures import ProcessPoolExecutor
import re


def _trim_preface_of_body(sample: str) -> str:
    """Trim the redundant descriptions/symbols/'def' declaration before the function body.
    Please see my comments in sampler.LLM (in sampler.py).
    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.

    -Example sample (function & description generated by LLM):
    -------------------------------------
    This is the optimized function ...
    def priority_v2(...) -> ...:
        return ...
    This function aims to ...
    -------------------------------------
    -This function removes the description above the function's signature, and the function's signature.
    -The indent of the code is preserved.
    -Return of this function:
    -------------------------------------
        return ...
    This function aims to ...
    -------------------------------------
    """
    start_str = 'def priority'
    end_str = '    return'
    code_list = re.findall('```python(.*?)```', sample, flags=re.DOTALL)
    code_core = None
    for code in code_list:
        if '\n'+start_str in code:  # and code.count('\n'+end_str) == 1:
            code_core = code
            break
    if code_core is None:
        raise Exception('Can not find core code.')
    lines = code.splitlines()
    func_body_lineno = 0
    find_def_declaration = False
    for lineno, line in enumerate(lines):
        # find the first 'def' statement in the given code
        if line.startswith(start_str):
            func_body_lineno = lineno
            find_def_declaration = True
            break
    if find_def_declaration:
        code = ''
        for line in lines[func_body_lineno + 1:]:
            code += line + '\n'
            if line.startswith(end_str):
                break
        return code
    return sample


def request(prompt):
    for retry_i in range(5):
        try:
            print('request...')
            # json_data = {
            #     "model": "llama3.3",
            #     "prompt": prompt,
            #     # "n_predict": 512
            #     "stream": False
            # }
            # # response = requests.post('http://114.214.164.112:8080/completion', json=json_data)
            # # response = json.loads(response.text)['content']
            # response = requests.post('http://localhost:11434/api/generate', json=json_data)
            # response = json.loads(response.text)['response']
            # return response   




            # url = "https://api.siliconflow.cn/v1/chat/completions"
            # payload = {
            #     "model": "Pro/deepseek-ai/DeepSeek-V3",
            #     "messages": [
            #         {
            #             "role": "system",
            #             "content": "You are a helpful assistant."
            #         },
            #         {
            #             "content": prompt,
            #             "role": "user"
            #         }
            #     ]
            # }
            # headers = {
            #     "Authorization": "Bearer sk-qkrxgebdsvhdrbeuarbsykhdxllsbshxlwlzqujujsqajnje",
            #     "Content-Type": "application/json"
            # }

            # response = requests.request("POST", url, json=payload, headers=headers)
            # if response.status_code != 200:
            #     print('response.status_code', response.status_code, response.text)
            # data = json.loads(response.text)

            # return data['choices'][0]['message']['content']


            headers = {
                'Authorization': 'Bearer f184bcd9-68b0-49be-8a3f-ea095ee71e14',
            }
            provider = 'DeepInfra'

            response = requests.post('https://ark.cn-beijing.volces.com/api/v3/chat/completions', headers=headers, json={
                'model': 'ep-20250227102412-tfkv8',  # v3
                # 'model': 'ep-20250303202036-j6hfh',  # r1
                'messages': [
                    {
                        "role": "system",
                        "content": "You are a helpful assistant."
                    },
                    {
                        "content": prompt,
                        "role": "user"
                    }
                ],
                # 'provider': {
                #     'order': [
                #         provider,
                #     ],
                #     'allow_fallbacks': False
                # },
            })

            if response.status_code != 200:
                print('response.status_code', response.status_code, response.text)
                raise Exception('request net error')

            

            data = json.loads(response.text)

            # with open('data.json', 'r') as f:
            #     data = json.load(f)

            # if data['provider'] != provider:
            #     print(f'specific provider: {provider}, actual provicer: {data["provider"]}')
            #     raise Exception('not the specific privoder')
            
            return data['choices'][0]['message']['content']
        except Exception as e:
            print(f'errr111__{retry_i}')
            print(e)
            time.sleep(1)
    return prompt


# def request_batch(prompt_batch):
#     for retry_i in range(5):
#         try:
#             print('request...')
#             json_data = {
#                 # "model": "llama3.3",
#                 "prompt": prompt_batch,
#                 "n_predict": 512
#                 # "stream": False
#             }
            
#             response = requests.post('http://114.214.164.112:8080/completion', json=json_data)
#             response_list = json.loads(response.text)
            
#             batch_result = [prompt+response['content'] for prompt, response in zip(prompt_batch, response_list)]
#             return batch_result   
#         except Exception as e:
#             print(f'errr111__{retry_i}')
#             print(e)
#             time.sleep(5)
#     return prompt_batch


class LLMAPI(sampler.LLM):
    """Language model that predicts continuation of provided source code.
    """

    def __init__(self, samples_per_prompt: int, trim=True):
        super().__init__(samples_per_prompt)
        # additional_prompt = ('Complete a different and more complex Python function. '
        #                      'Be creative and you can insert multiple if-else and for-loop in the code logic. '
        #                      'Only output the Python code, no descriptions.')
        additional_prompt = \
"""
Complete a different and more complex Python function. Be creative and you can insert multiple if-else and for-loop in the code logic. Only output the Python code, no descriptions. Evolve a better priority function in two ways:
  1. Propose a better strategy for deciding the priority in an online bin-packing problem.
  2. Identify places in the code that might offer tuning options. Wherever you see potential tuning parameters, wrap them in a tunable([...]) function call. Do not need the tunable function implementation. For example:
    - `if remaining_capacity > tunable([0.2, 0.5]):`
    - `sorted(items, key=lambda x: tunable([x.size, x.weight]))`
Prioritize strategy evolution first, then parameter tuning.
"""
        self._additional_prompt = additional_prompt
        self._trim = trim

        self.executor = ProcessPoolExecutor(max_workers=samples_per_prompt)
        

    def draw_samples(self, prompt: str) -> Collection[str]:
        """Returns multiple predicted continuations of `prompt`."""
        return self._draw_sample([prompt] * self._samples_per_prompt)

    def _draw_sample(self, content_list: list) -> str:
        prompt_list = ['\n'.join([content, self._additional_prompt]) for content in content_list]
        futures = [self.executor.submit(request, prompt) for prompt in prompt_list]
        response_list = []
        for future in futures:
            response = future.result()
            if self._trim:
                response = _trim_preface_of_body(response)
            response_list.append(response)
        return response_list
        # response_list = request_batch(prompt_list)
        # response_list_final = []
        # for response in response_list:
        #     if self._trim:
        #         response = _trim_preface_of_body(response)
        #         response_list_final.append(response)
        # return response_list_final


def _compile_and_run_function(program, function_to_run, function_to_evolve, dataset, numba_accelerate):
    try:
        # optimize the code (decorate function_to_run with @numba.jit())
        if numba_accelerate:
            program = evaluator_accelerate.add_numba_decorator(
                program=program,
                function_to_evolve=function_to_evolve
            )
        # compile the program, and maps the global func/var/class name to its address
        all_globals_namespace = {}
        # execute the program, map func/var/class to global namespace
        exec(program, all_globals_namespace)
        # get the pointer of 'function_to_run'
        function_to_run = all_globals_namespace[function_to_run]
        # return the execution results
        results = function_to_run(dataset)
        # the results must be int or float
        if not isinstance(results, (int, float)):
            return None, False
        return results, True
    except:
        # if raise any exception, we assume the execution failed
        return None, False


class Sandbox(evaluator.Sandbox):
    """Sandbox for executing generated code. Implemented by RZ.

    RZ: Sandbox returns the 'score' of the program and:
    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).
    2) stops the execution of the code in time (avoid endless loop).
    """

    def __init__(self, verbose=False, numba_accelerate=True):
        """
        Args:
            verbose         : Print evaluate information.
            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions
                              support numba acceleration, such as np.piecewise().
        """
        self._verbose = verbose
        self._numba_accelerate = numba_accelerate
        self.executor = ProcessPoolExecutor(max_workers=min(os.cpu_count(), 64))

    def run(
            self,
            program_list: list[str],
            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')
            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.
            inputs: Any,  # refers to the dataset
            test_input: str,  # refers to the current instance
            timeout_seconds: int,
            **kwargs  # RZ: add this
    ) -> tuple[Any, bool]:
        """Returns `function_to_run(test_input)` and whether execution succeeded.

        RZ: If the generated code (generated by LLM) is executed successfully,
        the output of this function is the score of a given program.
        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.
        """
        print(f'launch {len(program_list)} evaluate tasks')

        dataset = inputs[test_input]
        futures = [self.executor.submit(_compile_and_run_function, program, function_to_run, function_to_evolve, dataset, self._numba_accelerate) for program in program_list]
        result_list = []
        for future in futures:
            result = future.result()
            result_list.append(result)

        # print(f'evaluate tasks done')
        return result_list
        
        # result_queue = multiprocessing.Queue()
        # process = multiprocessing.Process(
        #     target=self._compile_and_run_function,
        #     args=(program, function_to_run, function_to_evolve, dataset, self._numba_accelerate, result_queue)
        # )
        # process.start()
        # process.join(timeout=timeout_seconds)
        # if process.is_alive():
        #     # if the process is not finished in time, we consider the program illegal
        #     process.terminate()
        #     process.join()
        #     results = None, False
        # else:
        #     if not result_queue.empty():
        #         results = result_queue.get_nowait()
        #     else:
        #         results = None, False

        # if self._verbose:
        #     print(f'================= Evaluated Program =================')
        #     program_: code_manipulation.Program = code_manipulation.text_to_program(text=program)
        #     func_to_evolve_: str = kwargs.get('func_to_evolve', 'priority')
        #     function_: code_manipulation.Function = program_.get_function(func_to_evolve_)
        #     function_: str = str(function_).strip('\n')
        #     print(f'{function_}')
        #     print(f'-----------------------------------------------------')
        #     print(f'Score: {str(results)}')
        #     print(f'=====================================================')
        #     print(f'\n\n')

        # return results



specification = r'''
import numpy as np


def get_valid_bin_indices(item: float, bins: np.ndarray) -> np.ndarray:
    """Returns indices of bins in which item can fit."""
    return np.nonzero((bins - item) >= 0)[0]


def online_binpack(
        items: tuple[float, ...], bins: np.ndarray
) -> tuple[list[list[float, ...], ...], np.ndarray]:
    """Performs online binpacking of `items` into `bins`."""
    # Track which items are added to each bin.
    packing = [[] for _ in bins]
    # Add items to bins.
    for item in items:
        # Extract bins that have sufficient space to fit item.
        valid_bin_indices = get_valid_bin_indices(item, bins)
        # Score each bin based on heuristic.
        priorities = priority(item, bins[valid_bin_indices])
        # Add item to bin with highest priority.
        best_bin = valid_bin_indices[np.argmax(priorities)]
        bins[best_bin] -= item
        packing[best_bin].append(item)
    # Remove unused bins from packing.
    packing = [bin_items for bin_items in packing if bin_items]
    return packing, bins


@funsearch.run
def evaluate(instances: dict) -> float:
    """Evaluate heuristic function on a set of online binpacking instances."""
    # List storing number of bins used for each instance.
    num_bins = []
    # Perform online binpacking for each instance.
    for name in instances:
        instance = instances[name]
        capacity = instance['capacity']
        items = instance['items']
        # Create num_items bins so there will always be space for all items,
        # regardless of packing order. Array has shape (num_items,).
        bins = np.array([capacity for _ in range(instance['num_items'])])
        # Pack items into bins and return remaining capacity in bins_packed, which
        # has shape (num_items,).
        _, bins_packed = online_binpack(items, bins)
        # If remaining capacity in a bin is equal to initial capacity, then it is
        # unused. Count number of used bins.
        num_bins.append((bins_packed != capacity).sum())
    # Score of heuristic function is negative of average number of bins used
    # across instances (as we want to minimize number of bins).
    return -np.mean(num_bins)


@funsearch.evolve
def priority(item: float, bins: np.ndarray) -> np.ndarray:
    """Returns priority with which we want to add item to each bin.

    Args:
        item: Size of item to be added to the bin.
        bins: Array of capacities for each bin.

    Return:
        Array of same size as bins with priority score of each bin.
    """
    ratios = item / bins
    log_ratios = np.log(ratios)
    priorities = -log_ratios
    return priorities
'''

# It should be noted that the if __name__ == '__main__' is required.
# Because the inner code uses multiprocess evaluation.
if __name__ == '__main__':
    class_config = config.ClassConfig(llm_class=LLMAPI, sandbox_class=Sandbox)
    config = config.Config(samples_per_prompt=1, evaluate_timeout_seconds=30)

    bin_packing_or3 = {'OR3': bin_packing_utils.datasets['OR3']}
    global_max_sample_num = 300  # if it is set to None, funsearch will execute an endless loop
    import shutil, os
    log_dir = f'logs'
    if os.path.exists(log_dir):
        # input('delete logs folder?')
        shutil.rmtree(log_dir)
        # time.sleep(1)
    funsearch.main(
        specification=specification,
        inputs=bin_packing_or3,
        config=config,
        max_sample_nums=global_max_sample_num,
        class_config=class_config,
        log_dir='logs/funsearch_llm_api',
    )
