import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import funsearch
from implementation import sampler
from implementation import evaluator_accelerate
from implementation import evaluator
from implementation import code_manipulation
from implementation import sample_llm_api
from bin_packing import bin_packing_utils

import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import sampler
import requests
import time
from concurrent.futures import ProcessPoolExecutor, TimeoutError
import re
import gc
import os
from config import config_type, log_dir, additional_prompt, specification


def _trim_preface_of_body(sample: str) -> str:
    """Trim the redundant descriptions/symbols/'def' declaration before the function body.
    Please see my comments in sampler.LLM (in sampler.py).
    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.

    -Example sample (function & description generated by LLM):
    -------------------------------------
    This is the optimized function ...
    def priority_v2(...) -> ...:
        return ...
    This function aims to ...
    -------------------------------------
    -This function removes the description above the function's signature, and the function's signature.
    -The indent of the code is preserved.
    -Return of this function:
    -------------------------------------
        return ...
    This function aims to ...
    -------------------------------------
    """
    start_str = 'def priority'
    end_str = '    return'
    lines = sample.splitlines()
    func_body_lineno = 0
    find_def_declaration = False
    for lineno in range(len(lines)-1, -1, -1):
        line = lines[lineno]
        # find the first 'def' statement in the given code
        if line.startswith(start_str):
            func_body_lineno = lineno
            find_def_declaration = True
            break
    if find_def_declaration:
        code = ''
        for line in lines[func_body_lineno + 1:]:
            code += line + '\n'
            if line.startswith(end_str):
                break
        return code
    return sample


def request(llm_ins: sample_llm_api.LLM, prompt: str):
    for retry_i in range(5):
        try:
            headers = {
                'Authorization': llm_ins.api_key,
            }

            json_data = {
                'model': llm_ins.model,
                'messages': [
                    {
                        "role": "system",
                        "content": "You are a helpful assistant."
                    },
                    {
                        "content": prompt,
                        "role": "user"
                    }
                ],
            }
            if llm_ins.provider:
                json_data['provider'] = {
                    'order': [
                        llm_ins.provider,
                    ],
                    'allow_fallbacks': False
                }

            response = requests.post(llm_ins.request_http, headers=headers, json=json_data)

            if response.status_code != 200:
                print('response.status_code', response.status_code, response.text)
                raise Exception('request net error')

            data = json.loads(response.text)

            if llm_ins.provider and data['provider'] != llm_ins.provider:
                print(f'specific provider: {llm_ins.provider}, actual provicer: {data["provider"]}')
                raise Exception('not the specific privoder')
            
            response_content = data['choices'][0]['message']['content']
            return llm_ins.llm_name, prompt, response_content
        except Exception as e:
            print(f'errr111__{retry_i}', llm_ins.llm_name, response.text)
            print(e)
            time.sleep(1)
    return llm_ins.llm_name, prompt, prompt


class LLMAPI(sampler.LLM):
    """Language model that predicts continuation of provided source code.
    """

    def __init__(self, samples_per_prompt: int, trim=True):
        super().__init__(samples_per_prompt)
        # additional_prompt = ('Complete a different and more complex Python function. '
        #                      'Be creative and you can insert multiple if-else and for-loop in the code logic. '
        #                      'Only output the Python code, no descriptions.')
#         additional_prompt = \
# """
# Complete a different and more complex Python function. Be creative and you can insert multiple if-else and for-loop in the code logic. Evolve a better priority function in two ways:
#   1. Propose a better strategy for deciding the priority in an online bin-packing problem.
#   2. Identify places in the code that might offer tuning options. Wherever you see potential tuning parameters, wrap them in a tunable([...]) function call. Do not need the tunable function implementation. For example:
#     - `if remaining_capacity > tunable([0.2, 0.5]):`
#     - `sorted(items, key=lambda x: tunable([x.size, x.weight]))`
# Prioritize strategy evolution first, then parameter tuning.
# """
#         additional_prompt = \
# """
# Create an improved Python function for online bin-packing that demonstrates:
# Novel priority strategy: Propose a smarter item-bin matching approach considering both spatial fit and future packing potential
# Parameter tuning points: Clearly mark tuning parameters using tunable([option1, option2, ...]) wrapper. Example:
# `if remaining_capacity > tunable([0.2, 0.5]):`
# `sorted(items, key=lambda x: tunable([x.size, x.weight]))`
# First focus on strategic innovation, then expose tuning parameters through tunable([option1, option2, ...]) calls. 
# You should keep implementation practical but non-trivial.
# Things you should also focus on:
# 1.You can try a lot of functions and check the answers, however, the final function you provide for me should be the best one.
# 2.You can merge the strategies of multiple algorithms, and try to improve the performance of the functions you generate by combining the advantages of multiple algorithms.
# 3.You can break the traditional thinking and try new ideas.That is to say, you can try any possible methods as long as they are correct and reasonable.
# """
        
        self._additional_prompt = additional_prompt
        self._trim = trim


    def draw_samples(self, llm_ins: sample_llm_api.LLM, prompt: str) -> Collection[str]:
        """Returns multiple predicted continuations of `prompt`."""
        return self._draw_sample(llm_ins, [prompt] * self._samples_per_prompt)

    def _draw_sample(self, llm_ins: sample_llm_api.LLM, content_list: list) -> str:
        prompt_list = ['\n'.join([content, self._additional_prompt]) for content in content_list]
        futures = [request(llm_ins, prompt) for prompt in prompt_list]
        response_list = []
        for future in futures:
            llm_name, prompt, response_ori = future
            if self._trim:
                try:
                    response = _trim_preface_of_body(response_ori)
                except Exception as err:
                    print('errrrrr response_ori', response_ori)
                    raise err
            response_list.append((llm_name, prompt, response_ori, response))
        return response_list


def _compile_and_run_function(program, function_to_run, function_to_evolve, dataset, numba_accelerate):
    try:
        # optimize the code (decorate function_to_run with @numba.jit())
        if numba_accelerate:
            program = evaluator_accelerate.add_numba_decorator(
                program=program,
                function_to_evolve=function_to_evolve
            )
        # compile the program, and maps the global func/var/class name to its address
        all_globals_namespace = {}
        # execute the program, map func/var/class to global namespace
        exec(program, all_globals_namespace)
        # get the pointer of 'function_to_run'
        function_to_run = all_globals_namespace[function_to_run]
        # return the execution results
        results = function_to_run(dataset)
        # the results must be int or float
        if not isinstance(results, (int, float)):
            return None, False
        return results, True
    except:
        # if raise any exception, we assume the execution failed
        return None, False


class Sandbox(evaluator.Sandbox):
    """Sandbox for executing generated code. Implemented by RZ.

    RZ: Sandbox returns the 'score' of the program and:
    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).
    2) stops the execution of the code in time (avoid endless loop).
    """

    def __init__(self, verbose=False, numba_accelerate=False):
        """
        Args:
            verbose         : Print evaluate information.
            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions
                              support numba acceleration, such as np.piecewise().
        """
        self._verbose = verbose
        self._numba_accelerate = numba_accelerate
        self.executor = ProcessPoolExecutor(max_workers=min(os.cpu_count(), 64))

    def run(
            self,
            program_list: list[str],
            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')
            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.
            inputs: Any,  # refers to the dataset
            test_input: str,  # refers to the current instance
            timeout_seconds: int,
            **kwargs  # RZ: add this
    ) -> tuple[Any, bool]:
        """Returns `function_to_run(test_input)` and whether execution succeeded.

        RZ: If the generated code (generated by LLM) is executed successfully,
        the output of this function is the score of a given program.
        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.
        """
        print(f'launch {len(program_list)} evaluate tasks')

        dataset = inputs[test_input]
        futures = [self.executor.submit(_compile_and_run_function, program, function_to_run, function_to_evolve, dataset, self._numba_accelerate) for program in program_list]
        result_list = []
        for future in futures:
            try:
                result = future.result(timeout=15)
                result_list.append(result)
            except TimeoutError:
                for pid in self.executor._processes:  # 访问内部进程PID
                    try:
                        os.kill(pid, 9)  # 强制杀死进程
                    except ProcessLookupError:
                        pass
                self.executor = ProcessPoolExecutor(max_workers=min(os.cpu_count(), 64))
                print('sanbox timeout errrr')
                return None, True
        return result_list, False


# It should be noted that the if __name__ == '__main__' is required.
# Because the inner code uses multiprocess evaluation.
if __name__ == '__main__':
    if config_type == 'bin_packing':
        inputs = {'OR3': bin_packing_utils.datasets['OR3']}
    elif config_type == 'cap_set':
        inputs = {'8': 8}
    else:
        raise Exception('wrong case')
    global_max_sample_num = 1000  # if it is set to None, funsearch will execute an endless loop
    import shutil, os
    if os.path.exists(log_dir):
        # input('delete logs folder?')
        shutil.rmtree(log_dir)
        # time.sleep(1)
    funsearch.main(
        specification=specification,
        inputs=inputs,
        max_sample_nums=global_max_sample_num,
        log_dir=f'{log_dir}/funsearch_llm_api',
        sandbox_class=Sandbox,
        llm_class=LLMAPI,
    )
