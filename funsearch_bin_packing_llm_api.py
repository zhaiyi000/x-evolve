import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import funsearch
from implementation import sampler
from implementation import evaluator_accelerate
from implementation import evaluator
from implementation import code_manipulation
from implementation import sample_llm_api
from bin_packing import bin_packing_utils
from cycle_graphs import cycle_graphs_utils

import json
import multiprocessing
from typing import Collection, Any
import http.client
from implementation import sampler
import requests
import time
from concurrent.futures import ProcessPoolExecutor, TimeoutError
import re
import gc
import os
from config import config_type, log_dir, additional_prompt, specification, measure_timeout, n_w_dim, n_dim, w_dim, island_cnt, nodes_dim
import random

print('pid', os.getpid())


def _trim_preface_of_body(sample: str) -> str:
    """Trim the redundant descriptions/symbols/'def' declaration before the function body.
    Please see my comments in sampler.LLM (in sampler.py).
    Since the LLM used in this file is not a pure code completion LLM, this trim function is required.

    -Example sample (function & description generated by LLM):
    -------------------------------------
    This is the optimized function ...
    def priority_v2(...) -> ...:
        return ...
    This function aims to ...
    -------------------------------------
    -This function removes the description above the function's signature, and the function's signature.
    -The indent of the code is preserved.
    -Return of this function:
    -------------------------------------
        return ...
    This function aims to ...
    -------------------------------------
    """
    start_str = 'def priority'
    end_str = '    return'
    lines = sample.splitlines()
    func_body_lineno = 0
    find_def_declaration = False
    for lineno in range(len(lines)-1, -1, -1):
        line = lines[lineno]
        # find the first 'def' statement in the given code
        if line.startswith(start_str):
            func_body_lineno = lineno + 1
            find_def_declaration = True
            break
    if find_def_declaration:
        code = ''

        comment_symbol = None
        line = lines[func_body_lineno].strip()
        if line.startswith("'''"):
            comment_symbol = "'''"
        elif line.startswith('"""'):
            comment_symbol = '"""'
        
        if comment_symbol:
            if line == comment_symbol:
                func_body_lineno += 1
            while True:
                line = lines[func_body_lineno].strip()
                func_body_lineno += 1
                if line.endswith(comment_symbol):
                    break
            
        while func_body_lineno < len(lines):
            code += lines[func_body_lineno] + '\n'
            func_body_lineno += 1
            if func_body_lineno < len(lines) and lines[func_body_lineno].strip() != '' and lines[func_body_lineno].startswith('    ') is False:
                break
        return code
    print('can not find core code')
    print(sample)
    print('can not find core code')
    raise Exception('can not find core code')


def request_inner(llm_ins: sample_llm_api.LLM, headers, json_data):
    response = requests.post(llm_ins.request_http, headers=headers, json=json_data)

    if response.status_code != 200:
        print('response.status_code', llm_ins.model, response.status_code, response.text)
        raise Exception('request net error')

    data = json.loads(response.text)

    if llm_ins.provider:
        try:
            data['provider']
        except Exception as err:
            print('no provider key')
            print(response.text)
            # if 'Model busy, retry later' in response.text:
            time.sleep(30)
            raise err

    if llm_ins.provider and data['provider'] != llm_ins.provider:
        print(f'specific provider: {llm_ins.provider}, actual provicer: {data["provider"]}')
        raise Exception('not the specific privoder')
    
    response_content = data['choices'][0]['message']['content']
    return response, response_content


def request(llm_ins: sample_llm_api.LLM, prompt: str):
    headers = {
        'Authorization': llm_ins.api_key,
    }
    temperature = random.uniform(0.1, 1.5)

    json_data = {
        'model': llm_ins.model,
        'messages': [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "content": prompt,
                "role": "user"
            }
        ],
        # 'temperature': 0.1
    }
    if llm_ins.provider:
        json_data['provider'] = {
            'order': [
                llm_ins.provider,
            ],
            'allow_fallbacks': False
        }

    response_1, response_content_1_ori = request_inner(llm_ins, headers, json_data)
    response_content_1 = _trim_preface_of_body(response_content_1_ori)

    return llm_ins.llm_name+'  '+str(temperature), prompt, (response_content_1_ori, response_content_1)


class LLMAPI(sampler.LLM):
    """Language model that predicts continuation of provided source code.
    """

    def __init__(self, samples_per_prompt: int, trim=True):
        super().__init__(samples_per_prompt)
        self._additional_prompt = additional_prompt
        self._trim = trim


    def draw_samples(self, llm_ins: sample_llm_api.LLM, prompt: str) -> Collection[str]:
        """Returns multiple predicted continuations of `prompt`."""
        return self._draw_sample(llm_ins, [prompt] * self._samples_per_prompt)

    def _draw_sample(self, llm_ins: sample_llm_api.LLM, content_list: list) -> str:
        prompt_list = [self._additional_prompt + '```python\n' + content + '```' for content in content_list]
        futures = [request(llm_ins, prompt) for prompt in prompt_list]
        return futures


import sys
import os
class HideOutput:
    """上下文管理器，用于抑制标准输出和标准错误"""
    def __enter__(self):
        self._original_stdout = sys.stdout
        self._original_stderr = sys.stderr
        sys.stdout = open(os.devnull, 'w')
        sys.stderr = open(os.devnull, 'w')
    def __exit__(self, exc_type, exc_val, exc_tb):
        sys.stdout.close()
        sys.stderr.close()
        sys.stdout = self._original_stdout
        sys.stderr = self._original_stderr


def _compile_and_run_function(program, function_to_run, function_to_evolve, dataset, numba_accelerate):
    try:
        # optimize the code (decorate function_to_run with @numba.jit())
        if numba_accelerate:
            program = evaluator_accelerate.add_numba_decorator(
                program=program,
                function_to_evolve=function_to_evolve
            )
        # compile the program, and maps the global func/var/class name to its address
        all_globals_namespace = {}
        # execute the program, map func/var/class to global namespace
        with HideOutput():
            exec(program, all_globals_namespace)
            # get the pointer of 'function_to_run'
            function_to_run = all_globals_namespace[function_to_run]
            # return the execution results
            results = function_to_run(dataset)
        # the results must be int or float
        if not isinstance(results, (int, float)):
            return None, False
        return results, True
    except:
        # if raise any exception, we assume the execution failed
        return None, False


class Sandbox(evaluator.Sandbox):
    """Sandbox for executing generated code. Implemented by RZ.

    RZ: Sandbox returns the 'score' of the program and:
    1) avoids the generated code to be harmful (accessing the internet, take up too much RAM).
    2) stops the execution of the code in time (avoid endless loop).
    """

    def __init__(self, verbose=False, numba_accelerate=False):
        """
        Args:
            verbose         : Print evaluate information.
            numba_accelerate: Use numba to accelerate the evaluation. It should be noted that not all numpy functions
                              support numba acceleration, such as np.piecewise().
        """
        self._verbose = verbose
        self._numba_accelerate = numba_accelerate
        self.executor = ProcessPoolExecutor(max_workers=min(os.cpu_count(), 64))

    def run(
            self,
            program_list: list[str],
            function_to_run: str,  # RZ: refers to the name of the function to run (e.g., 'evaluate')
            function_to_evolve: str,  # RZ: accelerate the code by decorating @numba.jit() on function_to_evolve.
            inputs: Any,  # refers to the dataset
            test_input: str,  # refers to the current instance
            timeout_seconds: int,
            **kwargs  # RZ: add this
    ) -> tuple[Any, bool]:
        """Returns `function_to_run(test_input)` and whether execution succeeded.

        RZ: If the generated code (generated by LLM) is executed successfully,
        the output of this function is the score of a given program.
        RZ: PLEASE NOTE THAT this SandBox is only designed for bin-packing problem.
        """
        # print(f'launch {len(program_list)} evaluate tasks')

        dataset = inputs[test_input]
        futures = [self.executor.submit(_compile_and_run_function, program, function_to_run, function_to_evolve, dataset, self._numba_accelerate) for program in program_list]
        result_list = []
        for future in futures:
            try:
                result = future.result(timeout=measure_timeout)
                result_list.append(result)
            except TimeoutError:
                for pid in self.executor._processes:  # 访问内部进程PID
                    try:
                        os.kill(pid, 9)  # 强制杀死进程
                    except ProcessLookupError:
                        pass
                self.executor = ProcessPoolExecutor(max_workers=min(os.cpu_count(), 64))
                # print('sanbox timeout errrr')
                return None, True
        return result_list, False


# It should be noted that the if __name__ == '__main__' is required.
# Because the inner code uses multiprocess evaluation.
if __name__ == '__main__':
    if config_type == 'bin_packing':
        inputs = {'OR3': bin_packing_utils.datasets['OR3']}
    elif config_type == 'cap_set':
        inputs = {n_dim: n_dim}
    elif config_type == 'cycle_graphs':
        inputs = {nodes_dim: cycle_graphs_utils.datasets[nodes_dim]}
    elif config_type == 'admissible_set':
        inputs = {'12_7': {'n': 12, 'w': 7}}
    elif config_type == 'symmetry_admissible_set':
        inputs = {n_w_dim: {'n': n_dim, 'w': w_dim}}
    elif config_type == 'corners':
        inputs = {n_dim: n_dim}
    else:
        raise Exception('wrong case')
    global_max_sample_num = island_cnt * 100000  # if it is set to None, funsearch will execute an endless loop
    
    import shutil, os
    if os.path.exists(log_dir):
        # y_or_n = input('delete logs folder? [n]')
        # if y_or_n == 'y':
        print('delete logs folder!')
        shutil.rmtree(log_dir)
    
    funsearch.main(
        specification=specification,
        inputs=inputs,
        max_sample_nums=global_max_sample_num,
        log_dir=f'{log_dir}/funsearch_llm_api',
        sandbox_class=Sandbox,
        llm_class=LLMAPI,
    )
